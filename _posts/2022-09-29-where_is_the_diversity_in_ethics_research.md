# Where is the diversity in ethics research?

What comes to your mind when you think about ethics in AI? Just take a couple of seconds to think about it. Most probably the first answer that appears are various examples of bias in ai, in the field of some kind of discrimination, like gender or skin color. On second thought you may think about privacy and data protection issues, that in turn may lead to unfavorable outcomes to individuals or groups. And then, a looong way down we reach the category of other ethical topics which are carelessly parked in the category "other". 

Just to give you an estimate: searching for “privacy in ai” on Google Scholar revealed almost 3.7 million results, “bias in ai” revealed over 2.5 million results, “secure ai” was still popular with 1.5 million results; “liability for ai” 330 thousand, “warfare ai” 280 thousand, “goal misalignment ai” 45 thousand, and “superintelligence” only around 15.000 results.

Privacy and security issues are among the leading research topics, as information in the digital world generally offers humans the potential to harm a great number of other beings simultaneously (and with less and less need for technical knowledge and equipment to be able to do so). For example the Ethics Research paper by among others Timnit Gebru and Emily M. Bender (which still remains undisclosed, but you find an article about its contents [here]( https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/)) outlines a number of different ethical concerns in relation to Googles language models, among others reduced explainability and risks that these models could be used for manipulative or criminal actions. 
Research on bias in AI addresses the question of how bias in data collection and transformation process may lead to predictions that show lower accuracy for certain groups, such as the very popular projects of [Joy Boulamwini and Timnit Gebru on face detection]( https://www.media.mit.edu/people/joyab/overview/) or [crime prediction algorithms]( https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), or the [Amazon algorithm that was alleged to discriminate on gender]( https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G). 

It may help us understand decision patterns, and it receives widespread media coverage and attention since it is very well suited to produce simple marvel-style the-good-people-and-the-big-tech-villain-narratives, but in this specific situation we would like to circumvent the spotlight and have a closer look at the less popular research topics. Here is, where the complexity starts. Here is, where you can descend to the depths of the human and non-human essence of being. 
So what kind of research do we miss here? All of the above is based on a certain set of assumptions. Assumptions, that are made about what “is good”, what “is bad”, what “is fair” etc. Moreover, it assumes, that there is general agreement among all humans on this, or that such an agreement can be reached. Research from the studies of [The Moral Machine]( https://www.moralmachine.net/) however indicates, that there is no uniform agreement on moral “rules”. But how can you tell an AI agent which outcome to achieve, if there is no alignment of what that good outcome may be?

And even if this specific question may be as old as social human life itself, and humans have been quarreling about this for a long time now, postulating theories and proving them wrong. But now a new agent just entered the round. Unrecognized by many, he I silent, only observing and listening to the common murmur. But those few, that have noticed a difference, and they have raised their voice and their concerns. A group of researchers predicted in 2017 that with a 50% chance AI will exceed human capabilities in most jobs [by 2060]( https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/). 

This development now raises a new field in ethics research, which deals with the interaction between human intelligence and non-human intelligence (meaning a high level general intelligence like human-level, or superintelligence). Even if humans achieve an agreement on what ethical standards and goals should be universal, how will they achieve agreement on these standards with a non-human intelligence (e.g. [goal misalignment problem]( https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/))? Will there be universal concepts that apply to human and non-human intelligent life (like will AI have the capability to suffer or have a conscience)? Will there be universal moral guidelines for both, human and non-human intelligence (like liability or personality)? And, finally, in a future world of technologies like nano-robotics or DNA-altering, what will be the concept of what makes a human, if any?

Another assumption that is secretly made, is that humans will be the ones in the driver’s seat. This must not necessarily be true once computational intelligence arises above human intelligence and evade our control mechanisms. The research group also predicted that, if an AI general/super intelligence develops, it may lead to a 5% chance of a catastrophic outcome, and updated it recently even to [10%]( https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/).
So, while eliminating bias from hiring models may serve the benefit of some groups at the cost of others, it may not necessarily lead us in a direction, where we gain the capability to mitigate risks to come in a (not so distant) future. I think that far more research needs to be put into serving challenges that put humanity as a whole at risk. More diversity needs to be introduced in ethics research (or at least the coverage by mainstream media), but diversity in my case does not refer to the gender and skin color of the researchers but addresses the topics and ideas of researchers. 


