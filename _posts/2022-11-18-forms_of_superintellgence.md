# If superintelligence emerges in our world, what will it be like? 

Generally speaking, when we say superintelligence, or more precisely superintelligence in the form of Artificial General Intelligence (AGI), we mean that 
machines outperform the human level of intelligence in a broad range of cognitive domains.
<br><br>
<sub> Note that I refer here to a definition of intelligence that is based on cognitive capabilities only. It is noncommittal regarding qualia: whether a 
  superintelligence would have subjective conscious experience might matter greatly for some questions, but is not at our focus at the moment. </sub> 
<br><br>
We have already some forms of ANI (Artificial Narrow Intelligence) out in the world that outperform human capabilities in specific areas like 
[DeepMind’s Aplha Go]( https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) or 
[speech recognition models](https://techxplore.com/news/2020-10-ai-outperforms-humans-speech-recognition.html). 
But an AGI differs in that it will be able to outperform human abilities in many domains, as many definitions agree. It will be like a human, just better. 
<br><br>
But... better how exactly? 
<br><br>
As human beings, our imagination is somewhat limited to concepts that we know – and as the most intelligent species we know on earth is actually *us, humans*, 
it is no surprise that we also picture superintelligence to be a lot like us. If you look at popular sci-fi movies, many superintelligences are some kind of 
robots or humanoid aliens, that more or less look and behave like a human, and are just somewhat stronger and/or eviler (and on average far away from 
super-intelligence). 
<br><br>
If we want to imagine a superintelligence, what would it be like? What does outperformance actually mean? How would it behave? What would it look like? 
How would it think (if at all)? How would it feel to be superintelligent? 
<br><br>
One thing to get closer to answering these questions is to look at what kind of forms superintelligence may take, and which we will do in the following. 
In his book “Superintelligence – Paths, Dangers, Strategies” Oxford Professor Nick Bostrom assumes that superintelligence may take three possible forms:
<br><br>
1.	Speed superintelligence
<br><br>
This machine intelligence is like a human, but just much much faster. Imagine a brain emulation of a doctor on a machine that runs 10,000 times faster than a 
human brain. To this individual, the world around him would unfold at an unbelievably slow speed. It would take him only a few seconds to read a scientific paper, 
read the case history of a couple of patients, check all x-rays and recommend a treatment. If the speedup factor is increased to one million, this individual could 
handle probably every patient in the world within one working day. 
<br><br>
This individual would prefer to live in a digital world, where other objects also operate at a similar speed. Alternatively, he would interact with the material 
world on a nano-scale, e.g. with nano bots or nano limbs, because (without getting into physics details) the smaller your tools, the faster they can move. 
He would prefer “to commune mainly with other fast minds rather than with brady-telic molasses-like humans”. 
<br><br>
Is such a speed realizable? In a human brain, information is carried at a speed of up to 120m/s (speed at which axons in brains can communicate). Electronic 
processors can communicate at an ideal speed of around 300,000,000m/s (speed of light), which is a speedup factor of a magnitude quite above one million. Also, 
the speed of computational elements in a microprocessor is currently 5-6 GHz and thus much faster than that of a human brain (200 Hz). Therefore, just a speedup 
based on a system “changing your hardware” from brain tissue to electric machines can lead to an immense boost in intelligence. 
<br><br>
Just speed alone might be not sufficient, as, with new information incoming, old information needs to be forgotten unless more storage and retrieval processes are 
added. Human brains are limited in this regard (you cannot grow yourself a second brain), but machines are easily scalable (you can add additional servers and 
processing units), therefore, it may be likely that an initial speedup in intelligence may result in a quality increase of intelligence quite easily. 
<br><br>
2.	Collective superintelligence
<br><br>
Another form of superintelligence may be a system where all its parts (minor intelligences) together create more than just the sum of these parts, a collective 
intelligence. This concept is actually quite familiar to us humans, as we already witnessed some sort of collective intelligence in human history: no engineer alone 
can build a space shuttle, and the shuttle wouldn’t fly if we hadn’t had a significant number of mathematicians and philosophers developing theories for aircraft in 
human history. Collective (super)intelligence can solve problems that can be broken up into tasks, on which specialized labor forces work simultaneously 
(like building a space shuttle or a city). Firms, work teams, universities, and countries, all are a form of collective intelligence that achieves above 
single-human-level performance (admittingly, at a varying degree of efficiency). 
<br><br>
But how much more super should the superintelligence be to be classified as superintelligence? If we look at human history, we could proudly pat our shoulders and 
say, that compared to a village of humans 500 years ago, we are already a collective superintelligence: we have invented phones, surgeries, aircraft, computers, 
spacecraft, and ice cream – all things the historian version of us 500 years ago could only dream of (or actually would probably lack the imagination to dream of I'd say). 
We could choose the present day as the baseline, and then it would be quite a big step from current collective intelligence to superintelligence. 
<br><br>
One can easily imagine, that current-level ANIs work together as specialized labor forces on tasks that result from the breakup of a more complex problem. For example, 
a collective intelligent engineering company, that builds a space shuttle on its own using autonomous robots would probably be regarded as a human-level intelligent, 
not a super superintelligent one, because we have already achieved this goal in human society. For gaining superintelligence relating to our present-day level, we 
would probably require another super-sized boost in intelligence similar to the invention of spoken language, writing, or printing. 
<br><br>
I, personally, could imagine such a superintelligence to be achieved in a form of super-fast communication of insights and understandings. We have already achieved 
some super-fast communication where messages and discoveries can be largely made available due to the invention of the web (at least to individuals with internet 
access). But we spend an awful waste of time (20-30 years per individual or even more) to get a grip on current theories and understanding of the world in school, 
universities and work. If I would read new literature about quantum mechanics, it doesn’t necessarily mean that I am able to understand what it actually means. I 
probably would need to study for quite an amount of years to conquer every step of basic physic and mathematic knowledge that needs to be understood first, before 
getting the ladder up to quantum mechanics (and also I need to have the cognitive capabilities to draw the correct conclusions from these, which is another quality 
of difficulty).
<br><br>
In my opinion, one way to achieve a level of a collective superintelligence could lie in the enhancement of information transfer in a way that not only facts are 
made available, but insights. Imagine, you would receive a paper about quantum mechanics, but it would include all “metadata”, i.e. all concepts that are necessary 
to know down to the basic math concepts – and you would gain an immediate understanding, a bit like in the Matrix Movie with Neo saying: “I know Kung-Fu.” Traditional 
schooling and universities wouldn’t be necessary anymore, all individuals would have instantly the same level of knowledge and would be updated as new research comes 
in (at least for these individuals that are favorite enough to make use of the technology, most probably digital individuals). Yeah, school’s out forever! 
<br><br>
3.	Quality superintelligence
<br><br>
Finally, the third imaginable form of superintelligence can be distinguished by quality. This form of superintelligence runs not only faster but is vastly 
qualitatively smarter. Obviously, the definition is a bit more fuzzier than on the two forms before, because this form is also one that is most difficult to 
imagine, as we do not have experience with intelligences at a qualitatively higher scale yet. 
<br><br>
You can try to conceive of such a situation: let’s take our closest and most intelligent relatives (noting here that we regard this from a humanly relevant cognitive 
perspective): chimpanzees. Would you try to explain to them the technology of a smartphone (if you could) and teach them to be engineers and to build smartphones? No, 
because nonhuman animals lack complex language, use only very basic tooling, are not able to make long-term plans, and have only limited abstract reasoning ability. 
The quality of their intelligence does not allow them to understand even the basic concepts of physics, and even much less those of electricity, microprocessors, and 
LEDs. 
<br><br>
Now, imagine the same situation with us in the role of chimpanzees and the superintelligence trying to explain to us its superintelligent gadgets. We wouldn’t not 
only be unable to understand how these are constructed, but we may also be even unable to understand for what use they are made. Superintelligence would be at least 
as much superior to us as our intelligence is superior to chimpanzees, dolphins, or elephants. 
<br><br>
Even though much of our technological gains are due to collective intelligence, the main difference in our abilities compared to non-human animals is due to our 
brains’ structures. We do understand complex problem-solving tasks and language because our brains evolved to understand these. Now, there might be an indefinite 
level of other abilities that can be gained, just by growing the right synapse connections in our brain to gain the understanding of these. And we haven’t yet, just 
because these weren’t of evolutionary benefits to us. 
<br><br>
This leaves us with a bit of a frustrating situation, similar to a blind person being unable to imagine the colorfulness of a sundown to a color-blind person, we 
may not even be able to imagine what kind of undiscovered abilities a superintelligence might develop, that we just lack the brain synapses (and the organs) to make 
sense of. 
<br><br>
Putting it all together, we can now imagine that a superintelligence might be in no way a human-like-looking robot unless it is one of the myriad agents of minor 
intelligence of a collective system. Even with regard to aliens, probably if these species look like humans, they would be of similar intelligence to humans (if they 
do not hide some additional brain tissue somewhere else in their octopus tentacles). Probably, superintelligent beings would look to us quite boring, like big server 
villages in basements, and lead digital lives in digital communities (if at all) at superhuman speed. 
<br><br>
Just remember this, when you watch the next sci-fi movie. 
